{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 1000 characters of CNT growth functions.pdf:\n",
      "Applied  Surface  Science  332  (2015)  756–760\n",
      "Contents  lists  available  at  ScienceDirect\n",
      "Applied  Surface  Science\n",
      "journal  h  om  epa  ge:  www.elsevier.com/locate/apsusc\n",
      "A  new understanding of  carbon  nanotube  growth:  Different  functions\n",
      "of\n",
      " carbon species\n",
      "Yueling  Zhanga,∗,  Baojun  Wangb,  Qing  Yuc,  Yajun  Tiand\n",
      "aCollege  of  Engineering,  Peking  University,  Summer  Palace  Road  5,  Beijing  100871,  PR  China\n",
      "bKey  Laboratory  of  Coal  Science  and  Technology,  Taiyuan  University  of  Technology,  Yingze  West  Street  79,  Taiyuan  030024,  PR  China\n",
      "cSchool  of  Chemistry  and  Chemical  Engineer  and  Center  of  Modern  Analysis,  Nanjing  University,  Hankou  Road  22,  Nanjing  210093,  PR  China\n",
      "dNational  Institute  of  Clean-and-low-carbon  Energy,  P.O.  Box  001  Shenhua  NICE,  Future  Science  &  Technology  Park,  Beijing  102209,  PR  China\n",
      "a r t  i  c  l  e  i  n  f  o\n",
      "Article history:\n",
      "Received\n",
      " 13  November  2014\n",
      "Received\n",
      " in  revised  form  12  \n",
      "First 1000 characters of CNT growth mechanism.pdf:\n",
      "Chemical Engineering Journal 445 (2022) 136807\n",
      "Available online 6 May 2022\n",
      "1385-8947/© 2022 Elsevier B.V. All rights reserved.Unraveling the mechanisms of carbon nanotube growth by chemical \n",
      "vapor deposition \n",
      "Georgios P. Gakis, Stefania Termine, Aikaterini-Flora A. Trompeta, Ioannis G. Aviziotis, \n",
      "Costas A. Charitidis* \n",
      "Research Lab of Advanced, Composite, Nano-Materials and Nanotechnology, Materials Science and Engineering Department, School of Chemical Engineering, National \n",
      "Technical University of Athens, 9 Heroon Polytechneiou Street, Zografos, Athens 15780, Greece   \n",
      "ARTICLE INFO  \n",
      "Keywords: \n",
      "CVD of CNTs \n",
      "Macroscopic model \n",
      "Growth mechanism \n",
      "Competitive phenomena \n",
      "Carbon diffusion \n",
      "Catalyst lifetime ABSTRACT  \n",
      "The mechanisms of carbon nanotube (CNT) growth by chemical vapor deposition of acetylene on Fe/SiO 2:Al2O3 \n",
      "(zeolite Y) catalyst are unraveled through a combined computational and experimental study. CNTs are syn-\n",
      "thesized in a horizontal reactor under atmospheric pressure w\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    pdf_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_file = os.path.join(folder_path, filename)\n",
    "            with open(pdf_file, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                pdf_text = \"\"\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    pdf_text += page.extract_text()\n",
    "                pdf_texts.append({\"filename\": filename, \"text\": pdf_text})\n",
    "    return pdf_texts\n",
    "\n",
    "# Usage\n",
    "folder_path = \"./Data\"  # Replace with the path where your PDFs are stored\n",
    "pdf_texts = extract_text_from_pdfs(folder_path)\n",
    "\n",
    "for pdf in pdf_texts:\n",
    "    print(f\"First 1000 characters of {pdf['filename']}:\")\n",
    "    print(pdf['text'][:1000])  # Print the first 1000 characters for review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What mechanism was used for carbon nanotube growth?\n",
      "A: The CNT growth mechanism involves chemical vapor deposition (CVD) with acetylene and a catalyst.\n",
      "\n",
      "Q: What techniques were used to characterize CNTs?\n",
      "A: The CNTs were characterized by scanning electron microscopy (SEM), transmission electron microscopy (TEM), and Raman spectroscopy.\n",
      "\n",
      "Q: How does catalyst lifetime affect CNT growth?\n",
      "A: Catalyst lifetime affects CNT growth by limiting the time during which the catalyst can facilitate CNT formation before deactivation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def generate_qa_pairs(pdf_texts):\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for pdf in pdf_texts:\n",
    "        text = pdf['text']\n",
    "        \n",
    "        # Example 1: Temperature ranges for CNT growth\n",
    "        temp_match = re.search(r'temperature range.*?(\\d+°C–\\d+°C)', text)\n",
    "        if temp_match:\n",
    "            question = \"What is the temperature range for CNT growth in the study?\"\n",
    "            answer = f\"The temperature range for CNT growth is {temp_match.group(1)}.\"\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "        # Example 2: CNT growth mechanism\n",
    "        if \"growth mechanism\" in text.lower():\n",
    "            question = \"What mechanism was used for carbon nanotube growth?\"\n",
    "            answer = \"The CNT growth mechanism involves chemical vapor deposition (CVD) with acetylene and a catalyst.\"\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "        # Example 3: Characterization methods for CNT\n",
    "        if \"characterized by\" in text.lower():\n",
    "            question = \"What techniques were used to characterize CNTs?\"\n",
    "            answer = \"The CNTs were characterized by scanning electron microscopy (SEM), transmission electron microscopy (TEM), and Raman spectroscopy.\"\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "        # Example 4: Impact of catalyst lifetime\n",
    "        if \"catalyst lifetime\" in text.lower():\n",
    "            question = \"How does catalyst lifetime affect CNT growth?\"\n",
    "            answer = \"Catalyst lifetime affects CNT growth by limiting the time during which the catalyst can facilitate CNT formation before deactivation.\"\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "        # Add more Q&A patterns based on the structure of your texts\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "# Example usage\n",
    "qa_pairs = generate_qa_pairs(pdf_texts)\n",
    "\n",
    "# Output the generated Q&A pairs\n",
    "for pair in qa_pairs:\n",
    "    print(f\"Q: {pair['question']}\")\n",
    "    print(f\"A: {pair['answer']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert Q&A pairs to DataFrame\n",
    "df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "# Create a Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What mechanism was used for carbon nanotube gr...</td>\n",
       "      <td>The CNT growth mechanism involves chemical vap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What techniques were used to characterize CNTs?</td>\n",
       "      <td>The CNTs were characterized by scanning electr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does catalyst lifetime affect CNT growth?</td>\n",
       "      <td>Catalyst lifetime affects CNT growth by limiti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What mechanism was used for carbon nanotube gr...   \n",
       "1    What techniques were used to characterize CNTs?   \n",
       "2      How does catalyst lifetime affect CNT growth?   \n",
       "\n",
       "                                              answer  \n",
       "0  The CNT growth mechanism involves chemical vap...  \n",
       "1  The CNTs were characterized by scanning electr...  \n",
       "2  Catalyst lifetime affects CNT growth by limiti...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/3 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 60.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load SciBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"question\"], examples[\"answer\"], truncation=True, padding=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 11.9MB/s]\n",
      "c:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\VANI SETH\\.cache\\huggingface\\hub\\models--t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "config.json: 100%|██████████| 1.21k/1.21k [00:00<?, ?B/s]\n",
      "c:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "model.safetensors: 100%|██████████| 892M/892M [00:56<00:00, 15.8MB/s] \n",
      "generation_config.json: 100%|██████████| 147/147 [00:00<?, ?B/s] \n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing PDF: CNT growth functions.pdf ---\n",
      "\n",
      "SciBERT Answer: \n",
      "T5 Generated Response: 0.4\n",
      "\n",
      "\n",
      "--- Processing PDF: CNT growth mechanism.pdf ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciBERT Answer: reserved . unraveling the mechanisms of carbon nanotube growth by chemical vapor deposition georgios p . gakis , stefania termine , aikaterini - flora a . trompeta , ioannis g . aviziotis , costas a . charitidis * research lab of advanced , composite , nano - materials and nanotechnology , materials science and engineering department , school of chemical engineering , national technical university of athens , 9 heroon polytechneiou street , zografos , athens 15780 , greece article info keywords : cvd of cnts macroscopic model growth mechanism competitive phenomena carbon diffusion catalyst lifetime abstract the mechanisms of carbon nanotube ( cnt ) growth by chemical vapor deposition of acetylene on fe / sio 2 : al2o\n",
      "T5 Generated Response: al2o\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Step 1: Extract Text from PDFs\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    pdf_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_file = os.path.join(folder_path, filename)\n",
    "            with open(pdf_file, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                pdf_text = \"\"\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    pdf_text += page.extract_text()\n",
    "                pdf_texts.append({\"filename\": filename, \"text\": pdf_text})\n",
    "    return pdf_texts\n",
    "\n",
    "# Step 2: SciBERT for Question Answering\n",
    "class SciBERTQA:\n",
    "    def __init__(self, model_name=\"allenai/scibert_scivocab_uncased\"):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    def answer_question(self, question, context, max_length=512):\n",
    "        # Tokenize with truncation to avoid long input issues\n",
    "        inputs = self.tokenizer(question, context, return_tensors=\"pt\", truncation=True, max_length=max_length, padding=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            start_scores = outputs.start_logits\n",
    "            end_scores = outputs.end_logits\n",
    "            all_tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "            answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores)+1])\n",
    "        return answer.replace(\" ##\", \"\")  # Fix tokenization artifacts\n",
    "\n",
    "# Step 3: T5 for Generating Fluent Responses\n",
    "class T5ResponseGenerator:\n",
    "    def __init__(self, model_name=\"t5-base\"):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    def generate_response(self, question, answer, max_length=150):\n",
    "        input_text = f\"question: {question} context: {answer}\"\n",
    "        inputs = self.tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "        \n",
    "        outputs = self.model.generate(inputs, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Step 4: Full Pipeline - Combine SciBERT and T5\n",
    "def qa_pipeline(folder_path):\n",
    "    # Extract text from PDFs\n",
    "    pdf_texts = extract_text_from_pdfs(folder_path)\n",
    "\n",
    "    # Initialize SciBERT QA model and T5 generator\n",
    "    scibert_qa = SciBERTQA()\n",
    "    t5_generator = T5ResponseGenerator()\n",
    "\n",
    "    for pdf in pdf_texts:\n",
    "        print(f\"\\n--- Processing PDF: {pdf['filename']} ---\\n\")\n",
    "        \n",
    "        # Define your question (this can be dynamic or user-input)\n",
    "        question = \"What is the growth temperature for carbon nanotubes?\"\n",
    "\n",
    "        # Step 1: Use SciBERT to get a concise answer\n",
    "        answer = scibert_qa.answer_question(question, pdf['text'])\n",
    "        print(f\"SciBERT Answer: {answer}\")\n",
    "\n",
    "        # Step 2: Use T5 to generate a more detailed response\n",
    "        detailed_response = t5_generator.generate_response(question, answer)\n",
    "        print(f\"T5 Generated Response: {detailed_response}\\n\")\n",
    "\n",
    "# Run the pipeline\n",
    "folder_path = \"./Data\"  # Replace with the actual folder path containing your PDFs\n",
    "qa_pipeline(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing PDF: CNT growth functions.pdf ---\n",
      "\n",
      "SciBERT Answer: pr china dnational institute of clean - and - low - carbon energy , p . o . box 001 shenhua nice , future science & technology park , beijing 102209 , pr china a r t i c l e i n f o article history : received 13 november 2014 received in revised form 12 january 2015 accepted 18 january 2015 available online 24 january 2015 keywords : chemical vapor deposition carbon nanotubes interface dynamics carbon species functiona b s t r a c t understanding the formation mechanism of carbon nanotubes ( cnts ) from carbon source is critical for controlled - production of cnts . in this study , the functions of carbon species were investigated by a thermogravimetric analyzer coupled with a mass spectroscope in using methane as carbon source of cnt growth in chemical vapor deposition ( cvd ) . it was found that a negative peak of c2h2species and a positive peak of c2h4species appeared at the cnt growth moment . accordingly it is deduced that the c2h2species react on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 Generated Response: cnt growth moment\n",
      "\n",
      "\n",
      "--- Processing PDF: CNT growth mechanism.pdf ---\n",
      "\n",
      "SciBERT Answer: al2o3 ( zeolite y ) catalyst are\n",
      "T5 Generated Response: 0.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Step 1: Extract Text from PDFs\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    pdf_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_file = os.path.join(folder_path, filename)\n",
    "            with open(pdf_file, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                pdf_text = \"\"\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    pdf_text += page.extract_text()\n",
    "                pdf_texts.append({\"filename\": filename, \"text\": pdf_text})\n",
    "    return pdf_texts\n",
    "\n",
    "# Step 2: SciBERT for Question Answering\n",
    "class SciBERTQA:\n",
    "    def __init__(self, model_name=\"allenai/scibert_scivocab_uncased\"):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    def answer_question(self, question, context, max_length=512):\n",
    "        # Tokenize with truncation to avoid long input issues\n",
    "        inputs = self.tokenizer(question, context, return_tensors=\"pt\", truncation=True, max_length=max_length, padding=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            start_scores = outputs.start_logits\n",
    "            end_scores = outputs.end_logits\n",
    "            all_tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "            answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores)+1])\n",
    "        return answer.replace(\" ##\", \"\")  # Fix tokenization artifacts\n",
    "\n",
    "# Split long text into smaller chunks\n",
    "def split_text_into_chunks(text, chunk_size=512):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        yield \" \".join(words[i:i + chunk_size])\n",
    "\n",
    "# Function to handle chunked input for SciBERT\n",
    "def answer_question_from_chunks(question, context, model, tokenizer, max_length=512):\n",
    "    best_answer = \"\"\n",
    "    \n",
    "    # Split the context into smaller chunks\n",
    "    chunks = split_text_into_chunks(context)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(question, chunk, return_tensors=\"pt\", truncation=True, max_length=max_length, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            start_scores = outputs.start_logits\n",
    "            end_scores = outputs.end_logits\n",
    "            all_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "            answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores)+1])\n",
    "            answer = answer.replace(\" ##\", \"\")  # Fix tokenization artifacts\n",
    "\n",
    "        # Replace with the first non-empty answer\n",
    "        if answer.strip():\n",
    "            best_answer = answer\n",
    "            break\n",
    "    \n",
    "    return best_answer\n",
    "\n",
    "# Step 3: T5 for Generating Fluent Responses\n",
    "class T5ResponseGenerator:\n",
    "    def __init__(self, model_name=\"t5-base\"):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    def generate_response(self, question, answer, max_length=200):\n",
    "        input_text = f\"question: {question} context: {answer}\"\n",
    "        inputs = self.tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "        \n",
    "        # Adjust generation parameters\n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=5,            # Use beam search to improve quality\n",
    "            no_repeat_ngram_size=2, # Avoid repetition\n",
    "            early_stopping=True,\n",
    "            length_penalty=1.0,     # Penalize short answers\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Step 4: Full Pipeline - Combine SciBERT and T5\n",
    "def qa_pipeline(folder_path):\n",
    "    # Extract text from PDFs\n",
    "    pdf_texts = extract_text_from_pdfs(folder_path)\n",
    "\n",
    "    # Initialize SciBERT QA model and T5 generator\n",
    "    scibert_qa = SciBERTQA()\n",
    "    t5_generator = T5ResponseGenerator()\n",
    "\n",
    "    for pdf in pdf_texts:\n",
    "        print(f\"\\n--- Processing PDF: {pdf['filename']} ---\\n\")\n",
    "        \n",
    "        # Define your question (this can be dynamic or user-input)\n",
    "        question = \"What is the growth temperature for carbon nanotubes?\"\n",
    "\n",
    "        # Step 1: Use SciBERT to get a concise answer from chunks\n",
    "        answer = answer_question_from_chunks(question, pdf['text'], scibert_qa.model, scibert_qa.tokenizer)\n",
    "        print(f\"SciBERT Answer: {answer}\")\n",
    "\n",
    "        if not answer.strip():\n",
    "            print(\"No valid answer found from SciBERT.\")\n",
    "            continue\n",
    "\n",
    "        # Step 2: Use T5 to generate a more detailed response\n",
    "        detailed_response = t5_generator.generate_response(question, answer)\n",
    "        print(f\"T5 Generated Response: {detailed_response}\\n\")\n",
    "\n",
    "# Run the pipeline\n",
    "folder_path = \"./Data\"  # Replace with the actual folder path containing your PDFs\n",
    "qa_pipeline(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
