{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 1000 characters of CNT growth functions.pdf:\n",
      "Applied  Surface  Science  332  (2015)  756–760\n",
      "Contents  lists  available  at  ScienceDirect\n",
      "Applied  Surface  Science\n",
      "journal  h  om  epa  ge:  www.elsevier.com/locate/apsusc\n",
      "A  new understanding of  carbon  nanotube  growth:  Different  functions\n",
      "of\n",
      " carbon species\n",
      "Yueling  Zhanga,∗,  Baojun  Wangb,  Qing  Yuc,  Yajun  Tiand\n",
      "aCollege  of  Engineering,  Peking  University,  Summer  Palace  Road  5,  Beijing  100871,  PR  China\n",
      "bKey  Laboratory  of  Coal  Science  and  Technology,  Taiyuan  University  of  Technology,  Yingze  West  Street  79,  Taiyuan  030024,  PR  China\n",
      "cSchool  of  Chemistry  and  Chemical  Engineer  and  Center  of  Modern  Analysis,  Nanjing  University,  Hankou  Road  22,  Nanjing  210093,  PR  China\n",
      "dNational  Institute  of  Clean-and-low-carbon  Energy,  P.O.  Box  001  Shenhua  NICE,  Future  Science  &  Technology  Park,  Beijing  102209,  PR  China\n",
      "a r t  i  c  l  e  i  n  f  o\n",
      "Article history:\n",
      "Received\n",
      " 13  November  2014\n",
      "Received\n",
      " in  revised  form  12  \n",
      "First 1000 characters of CNT growth mechanism.pdf:\n",
      "Chemical Engineering Journal 445 (2022) 136807\n",
      "Available online 6 May 2022\n",
      "1385-8947/© 2022 Elsevier B.V. All rights reserved.Unraveling the mechanisms of carbon nanotube growth by chemical \n",
      "vapor deposition \n",
      "Georgios P. Gakis, Stefania Termine, Aikaterini-Flora A. Trompeta, Ioannis G. Aviziotis, \n",
      "Costas A. Charitidis* \n",
      "Research Lab of Advanced, Composite, Nano-Materials and Nanotechnology, Materials Science and Engineering Department, School of Chemical Engineering, National \n",
      "Technical University of Athens, 9 Heroon Polytechneiou Street, Zografos, Athens 15780, Greece   \n",
      "ARTICLE INFO  \n",
      "Keywords: \n",
      "CVD of CNTs \n",
      "Macroscopic model \n",
      "Growth mechanism \n",
      "Competitive phenomena \n",
      "Carbon diffusion \n",
      "Catalyst lifetime ABSTRACT  \n",
      "The mechanisms of carbon nanotube (CNT) growth by chemical vapor deposition of acetylene on Fe/SiO 2:Al2O3 \n",
      "(zeolite Y) catalyst are unraveled through a combined computational and experimental study. CNTs are syn-\n",
      "thesized in a horizontal reactor under atmospheric pressure w\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    pdf_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_file = os.path.join(folder_path, filename)\n",
    "            with open(pdf_file, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                pdf_text = \"\"\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    pdf_text += page.extract_text()\n",
    "                pdf_texts.append({\"filename\": filename, \"text\": pdf_text})\n",
    "    return pdf_texts\n",
    "\n",
    "# Usage\n",
    "folder_path = \"./Data\"  # Replace with the path where your PDFs are stored\n",
    "pdf_texts = extract_text_from_pdfs(folder_path)\n",
    "\n",
    "for pdf in pdf_texts:\n",
    "    print(f\"First 1000 characters of {pdf['filename']}:\")\n",
    "    print(pdf['text'][:1000])  # Print the first 1000 characters for review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What mechanism was used for carbon nanotube growth?\n",
      "A: The CNT growth mechanism involves chemical vapor deposition (CVD) with acetylene and a catalyst.\n",
      "\n",
      "Q: What techniques were used to characterize CNTs?\n",
      "A: The CNTs were characterized by scanning electron microscopy (SEM), transmission electron microscopy (TEM), and Raman spectroscopy.\n",
      "\n",
      "Q: How does catalyst lifetime affect CNT growth?\n",
      "A: Catalyst lifetime affects CNT growth by limiting the time during which the catalyst can facilitate CNT formation before deactivation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def generate_qa_pairs(pdf_texts):\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for pdf in pdf_texts:\n",
    "        text = pdf['text']\n",
    "        \n",
    "        # Example 1: Temperature ranges for CNT growth\n",
    "        temp_match = re.search(r'temperature range.*?(\\d+°C–\\d+°C)', text)\n",
    "        if temp_match:\n",
    "            question = \"What is the temperature range for CNT growth in the study?\"\n",
    "            answer = f\"The temperature range for CNT growth is {temp_match.group(1)}.\"\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "        # Example 2: CNT growth mechanism\n",
    "        if \"growth mechanism\" in text.lower():\n",
    "            question = \"What mechanism was used for carbon nanotube growth?\"\n",
    "            answer = \"The CNT growth mechanism involves chemical vapor deposition (CVD) with acetylene and a catalyst.\"\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "        # Example 3: Characterization methods for CNT\n",
    "        if \"characterized by\" in text.lower():\n",
    "            question = \"What techniques were used to characterize CNTs?\"\n",
    "            answer = \"The CNTs were characterized by scanning electron microscopy (SEM), transmission electron microscopy (TEM), and Raman spectroscopy.\"\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "        # Example 4: Impact of catalyst lifetime\n",
    "        if \"catalyst lifetime\" in text.lower():\n",
    "            question = \"How does catalyst lifetime affect CNT growth?\"\n",
    "            answer = \"Catalyst lifetime affects CNT growth by limiting the time during which the catalyst can facilitate CNT formation before deactivation.\"\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "        # Add more Q&A patterns based on the structure of your texts\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "# Example usage\n",
    "qa_pairs = generate_qa_pairs(pdf_texts)\n",
    "\n",
    "# Output the generated Q&A pairs\n",
    "for pair in qa_pairs:\n",
    "    print(f\"Q: {pair['question']}\")\n",
    "    print(f\"A: {pair['answer']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert Q&A pairs to DataFrame\n",
    "df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "# Create a Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What mechanism was used for carbon nanotube gr...</td>\n",
       "      <td>The CNT growth mechanism involves chemical vap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What techniques were used to characterize CNTs?</td>\n",
       "      <td>The CNTs were characterized by scanning electr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does catalyst lifetime affect CNT growth?</td>\n",
       "      <td>Catalyst lifetime affects CNT growth by limiti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What mechanism was used for carbon nanotube gr...   \n",
       "1    What techniques were used to characterize CNTs?   \n",
       "2      How does catalyst lifetime affect CNT growth?   \n",
       "\n",
       "                                              answer  \n",
       "0  The CNT growth mechanism involves chemical vap...  \n",
       "1  The CNTs were characterized by scanning electr...  \n",
       "2  Catalyst lifetime affects CNT growth by limiti...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vocab.txt: 100%|██████████| 228k/228k [00:00<00:00, 7.45MB/s]\n",
      "c:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\VANI SETH\\.cache\\huggingface\\hub\\models--allenai--scibert_scivocab_uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "config.json: 100%|██████████| 385/385 [00:00<00:00, 388kB/s]\n",
      "Map:   0%|          | 0/3 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 794.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load SciBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"question\"], examples[\"answer\"], truncation=True, padding=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 442M/442M [00:28<00:00, 15.3MB/s] \n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForQuestionAnswering\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallenai/scibert_scivocab_uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Define training arguments\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Initialize the Trainer\u001b[39;00m\n\u001b[0;32m     20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     21\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     22\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     23\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset,\n\u001b[0;32m     24\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m     25\u001b[0m )\n",
      "File \u001b[1;32m<string>:111\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1340\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(version\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version) \u001b[38;5;241m==\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[0;32m   1335\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1339\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m-> 1340\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1341\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1342\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1343\u001b[0m ):\n\u001b[0;32m   1344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1351\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1357\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1764\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1761\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   1762\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1763\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 1764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     52\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1672\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   1671\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available(min_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.20.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1672\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   1673\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1674\u001b[0m         )\n\u001b[0;32m   1675\u001b[0m     AcceleratorState\u001b[38;5;241m.\u001b[39m_reset_state(reset_partial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, Trainer, TrainingArguments\n",
    "\n",
    "# Load SciBERT model for question answering\n",
    "model = BertForQuestionAnswering.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "                                              0.0/324.4 kB ? eta -:--:--\n",
      "     -------------                          112.6/324.4 kB 3.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 324.4/324.4 kB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.30.2)\n",
      "Collecting transformers[torch]\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "                                              0.0/9.5 MB ? eta -:--:--\n",
      "     -                                        0.5/9.5 MB 9.4 MB/s eta 0:00:01\n",
      "     -----                                    1.2/9.5 MB 11.1 MB/s eta 0:00:01\n",
      "     ---------                                2.2/9.5 MB 13.8 MB/s eta 0:00:01\n",
      "     ------------                             3.1/9.5 MB 15.0 MB/s eta 0:00:01\n",
      "     ----------------                         3.9/9.5 MB 15.6 MB/s eta 0:00:01\n",
      "     -------------------                      4.6/9.5 MB 15.6 MB/s eta 0:00:01\n",
      "     -----------------------                  5.5/9.5 MB 15.9 MB/s eta 0:00:01\n",
      "     -------------------------                6.1/9.5 MB 15.6 MB/s eta 0:00:01\n",
      "     --------------------------               6.3/9.5 MB 14.4 MB/s eta 0:00:01\n",
      "     -----------------------------            7.0/9.5 MB 14.4 MB/s eta 0:00:01\n",
      "     --------------------------------         7.7/9.5 MB 14.6 MB/s eta 0:00:01\n",
      "     -----------------------------------      8.4/9.5 MB 14.6 MB/s eta 0:00:01\n",
      "     --------------------------------------   9.1/9.5 MB 14.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  9.5/9.5 MB 14.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 9.5/9.5 MB 14.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (22.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (2.0.1)\n",
      "Collecting huggingface-hub>=0.21.0 (from accelerate)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl (417 kB)\n",
      "                                              0.0/417.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 417.5/417.5 kB 27.2 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.4.3 (from accelerate)\n",
      "  Downloading safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "                                              0.0/286.0 kB ? eta -:--:--\n",
      "     ------------------------------------- 286.0/286.0 kB 18.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (3.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2023.3.23)\n",
      "Requirement already satisfied: requests in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2.30.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers[torch])\n",
      "  Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "                                              0.0/2.2 MB ? eta -:--:--\n",
      "     -------------                            0.8/2.2 MB 16.3 MB/s eta 0:00:01\n",
      "     ------------------------                 1.4/2.2 MB 17.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.2/2.2 MB 15.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.6.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\vani seth\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, accelerate, transformers\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.3.1\n",
      "    Uninstalling safetensors-0.3.1:\n",
      "      Successfully uninstalled safetensors-0.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\VANI SETH\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\~afetensors\\\\_safetensors_rust.cp311-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install accelerate transformers[torch] -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m], examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Example: Assume tokenized_dataset has been created\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize the training arguments\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Initialize the Trainer\u001b[39;00m\n\u001b[0;32m     28\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     29\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     30\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     31\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset,\n\u001b[0;32m     32\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m     33\u001b[0m )\n",
      "File \u001b[1;32m<string>:111\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1340\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(version\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version) \u001b[38;5;241m==\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[0;32m   1335\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1339\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m-> 1340\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1341\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1342\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1343\u001b[0m ):\n\u001b[0;32m   1344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1351\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1357\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1764\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1761\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   1762\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1763\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 1764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     52\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1672\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   1671\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available(min_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.20.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1672\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   1673\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1674\u001b[0m         )\n\u001b[0;32m   1675\u001b[0m     AcceleratorState\u001b[38;5;241m.\u001b[39m_reset_state(reset_partial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, Trainer, TrainingArguments, BertTokenizer\n",
    "\n",
    "# Load SciBERT model for question answering\n",
    "model = BertForQuestionAnswering.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"question\"], examples[\"answer\"], truncation=True, padding=True)\n",
    "\n",
    "# Example: Assume tokenized_dataset has been created\n",
    "# Initialize the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk-IewRtt-d9VEzjxxBg6gvvh5Xziod8p0OfxRGXPqC2ET3BlbkFJMVpirURkiA2w-awN2_AnTPh-rhP_kLMeRMZnysHsIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing PDF: CNT growth functions.pdf ---\n",
      "\n",
      "SciBERT Answer: \n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 96\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[0;32m     95\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Data\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the actual folder path containing your PDFs\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m \u001b[43mqa_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 91\u001b[0m, in \u001b[0;36mqa_pipeline\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSciBERT Answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Get a detailed response from GPT-3.5-turbo\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m detailed_response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response_with_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT-3.5 Generated Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetailed_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 46\u001b[0m, in \u001b[0;36mgenerate_response_with_gpt\u001b[1;34m(question, answer)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_response_with_gpt\u001b[39m(question, answer):\n\u001b[0;32m     44\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCan you expand on this and provide a detailed explanation?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 46\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    761\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    764\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    765\u001b[0m     )\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import PyPDF2\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"sk-IewRtt-d9VEzjxxBg6gvvh5Xziod8p0OfxRGXPqC2ET3BlbkFJMVpirURkiA2w-awN2_AnTPh-rhP_kLMeRMZnysHsIA\" \n",
    "\n",
    "# Step 1: Extract Text from Multiple PDFs\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    pdf_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_file = os.path.join(folder_path, filename)\n",
    "            with open(pdf_file, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                pdf_text = \"\"\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    pdf_text += page.extract_text()\n",
    "                pdf_texts.append({\"filename\": filename, \"text\": pdf_text})\n",
    "    return pdf_texts\n",
    "\n",
    "# Step 2: Use SciBERT for Question Answering\n",
    "class SciBERTQA:\n",
    "    def __init__(self, model_name=\"allenai/scibert_scivocab_uncased\"):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    def answer_question(self, question, context):\n",
    "        inputs = self.tokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            start_scores = outputs.start_logits\n",
    "            end_scores = outputs.end_logits\n",
    "            all_tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "            answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores)+1])\n",
    "        return answer.replace(\" ##\", \"\")  # Fix tokenization artifacts\n",
    "\n",
    "# Step 3: Use GPT-3.5-turbo to Generate Fluent Response\n",
    "def generate_response_with_gpt(question, answer):\n",
    "    prompt = f\"Question: {question}\\nAnswer: {answer}\\nCan you expand on this and provide a detailed explanation?\"\n",
    "    \n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=150,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "def answer_question_with_truncation(question, context, model, tokenizer, max_length=512):\n",
    "    # Tokenize the question and context with truncation\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, max_length=max_length, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        start_scores = outputs.start_logits\n",
    "        end_scores = outputs.end_logits\n",
    "        \n",
    "        # Get the answer tokens\n",
    "        all_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "        answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores)+1])\n",
    "        \n",
    "    return answer.replace(\" ##\", \"\")  # Fix tokenization artifacts\n",
    "\n",
    "# Now use this modified function in your QA pipeline\n",
    "def qa_pipeline(folder_path):\n",
    "    # Extract text from PDFs\n",
    "    pdf_texts = extract_text_from_pdfs(folder_path)\n",
    "\n",
    "    # Initialize SciBERT QA model\n",
    "    scibert_qa = SciBERTQA()\n",
    "\n",
    "    for pdf in pdf_texts:\n",
    "        print(f\"\\n--- Processing PDF: {pdf['filename']} ---\\n\")\n",
    "        \n",
    "        # Define your question\n",
    "        question = \"What is the growth temperature for carbon nanotubes?\"\n",
    "\n",
    "        # Get the answer from SciBERT with truncation\n",
    "        answer = answer_question_with_truncation(question, pdf['text'], scibert_qa.model, scibert_qa.tokenizer)\n",
    "        print(f\"SciBERT Answer: {answer}\")\n",
    "\n",
    "        # Get a detailed response from GPT-3.5-turbo\n",
    "        detailed_response = generate_response_with_gpt(question, answer)\n",
    "        print(f\"GPT-3.5 Generated Response: {detailed_response}\\n\")\n",
    "\n",
    "# Run the pipeline\n",
    "folder_path = \"./Data\"  # Replace with the actual folder path containing your PDFs\n",
    "qa_pipeline(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[0;32m     76\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Data\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the actual folder path containing your PDFs\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[43mqa_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 59\u001b[0m, in \u001b[0;36mqa_pipeline\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Initialize SciBERT QA model and T5 generator\u001b[39;00m\n\u001b[0;32m     58\u001b[0m scibert_qa \u001b[38;5;241m=\u001b[39m SciBERTQA()\n\u001b[1;32m---> 59\u001b[0m t5_generator \u001b[38;5;241m=\u001b[39m \u001b[43mT5ResponseGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pdf \u001b[38;5;129;01min\u001b[39;00m pdf_texts:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Processing PDF: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 42\u001b[0m, in \u001b[0;36mT5ResponseGenerator.__init__\u001b[1;34m(self, model_name)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-base\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1026\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1026\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1014\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1012\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Step 1: Extract Text from PDFs\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    pdf_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_file = os.path.join(folder_path, filename)\n",
    "            with open(pdf_file, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                pdf_text = \"\"\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    pdf_text += page.extract_text()\n",
    "                pdf_texts.append({\"filename\": filename, \"text\": pdf_text})\n",
    "    return pdf_texts\n",
    "\n",
    "# Step 2: SciBERT for Question Answering\n",
    "class SciBERTQA:\n",
    "    def __init__(self, model_name=\"allenai/scibert_scivocab_uncased\"):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    def answer_question(self, question, context, max_length=512):\n",
    "        # Tokenize with truncation to avoid long input issues\n",
    "        inputs = self.tokenizer(question, context, return_tensors=\"pt\", truncation=True, max_length=max_length, padding=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            start_scores = outputs.start_logits\n",
    "            end_scores = outputs.end_logits\n",
    "            all_tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "            answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores)+1])\n",
    "        return answer.replace(\" ##\", \"\")  # Fix tokenization artifacts\n",
    "\n",
    "# Step 3: T5 for Generating Fluent Responses\n",
    "class T5ResponseGenerator:\n",
    "    def __init__(self, model_name=\"t5-base\"):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    def generate_response(self, question, answer, max_length=150):\n",
    "        input_text = f\"question: {question} context: {answer}\"\n",
    "        inputs = self.tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "        \n",
    "        outputs = self.model.generate(inputs, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Step 4: Full Pipeline - Combine SciBERT and T5\n",
    "def qa_pipeline(folder_path):\n",
    "    # Extract text from PDFs\n",
    "    pdf_texts = extract_text_from_pdfs(folder_path)\n",
    "\n",
    "    # Initialize SciBERT QA model and T5 generator\n",
    "    scibert_qa = SciBERTQA()\n",
    "    t5_generator = T5ResponseGenerator()\n",
    "\n",
    "    for pdf in pdf_texts:\n",
    "        print(f\"\\n--- Processing PDF: {pdf['filename']} ---\\n\")\n",
    "        \n",
    "        # Define your question (this can be dynamic or user-input)\n",
    "        question = \"What is the growth temperature for carbon nanotubes?\"\n",
    "\n",
    "        # Step 1: Use SciBERT to get a concise answer\n",
    "        answer = scibert_qa.answer_question(question, pdf['text'])\n",
    "        print(f\"SciBERT Answer: {answer}\")\n",
    "\n",
    "        # Step 2: Use T5 to generate a more detailed response\n",
    "        detailed_response = t5_generator.generate_response(question, answer)\n",
    "        print(f\"T5 Generated Response: {detailed_response}\\n\")\n",
    "\n",
    "# Run the pipeline\n",
    "folder_path = \"./Data\"  # Replace with the actual folder path containing your PDFs\n",
    "qa_pipeline(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "                                              0.0/991.5 kB ? eta -:--:--\n",
      "     ---------                              245.8/991.5 kB 7.6 MB/s eta 0:00:01\n",
      "     ------------------------               634.9/991.5 kB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  983.0/991.5 kB 8.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 991.5/991.5 kB 6.3 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[0;32m     76\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Data\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the actual folder path containing your PDFs\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[43mqa_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 59\u001b[0m, in \u001b[0;36mqa_pipeline\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Initialize SciBERT QA model and T5 generator\u001b[39;00m\n\u001b[0;32m     58\u001b[0m scibert_qa \u001b[38;5;241m=\u001b[39m SciBERTQA()\n\u001b[1;32m---> 59\u001b[0m t5_generator \u001b[38;5;241m=\u001b[39m \u001b[43mT5ResponseGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pdf \u001b[38;5;129;01min\u001b[39;00m pdf_texts:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Processing PDF: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 42\u001b[0m, in \u001b[0;36mT5ResponseGenerator.__init__\u001b[1;34m(self, model_name)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-base\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1026\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1026\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\VANI SETH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1014\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1012\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Step 1: Extract Text from PDFs\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    pdf_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_file = os.path.join(folder_path, filename)\n",
    "            with open(pdf_file, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                pdf_text = \"\"\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    pdf_text += page.extract_text()\n",
    "                pdf_texts.append({\"filename\": filename, \"text\": pdf_text})\n",
    "    return pdf_texts\n",
    "\n",
    "# Step 2: SciBERT for Question Answering\n",
    "class SciBERTQA:\n",
    "    def __init__(self, model_name=\"allenai/scibert_scivocab_uncased\"):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    def answer_question(self, question, context, max_length=512):\n",
    "        # Tokenize with truncation to avoid long input issues\n",
    "        inputs = self.tokenizer(question, context, return_tensors=\"pt\", truncation=True, max_length=max_length, padding=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            start_scores = outputs.start_logits\n",
    "            end_scores = outputs.end_logits\n",
    "            all_tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "            answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores)+1])\n",
    "        return answer.replace(\" ##\", \"\")  # Fix tokenization artifacts\n",
    "\n",
    "# Step 3: T5 for Generating Fluent Responses\n",
    "class T5ResponseGenerator:\n",
    "    def __init__(self, model_name=\"t5-base\"):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    def generate_response(self, question, answer, max_length=150):\n",
    "        input_text = f\"question: {question} context: {answer}\"\n",
    "        inputs = self.tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "        \n",
    "        outputs = self.model.generate(inputs, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Step 4: Full Pipeline - Combine SciBERT and T5\n",
    "def qa_pipeline(folder_path):\n",
    "    # Extract text from PDFs\n",
    "    pdf_texts = extract_text_from_pdfs(folder_path)\n",
    "\n",
    "    # Initialize SciBERT QA model and T5 generator\n",
    "    scibert_qa = SciBERTQA()\n",
    "    t5_generator = T5ResponseGenerator()\n",
    "\n",
    "    for pdf in pdf_texts:\n",
    "        print(f\"\\n--- Processing PDF: {pdf['filename']} ---\\n\")\n",
    "        \n",
    "        # Define your question (this can be dynamic or user-input)\n",
    "        question = \"What is the growth temperature for carbon nanotubes?\"\n",
    "\n",
    "        # Step 1: Use SciBERT to get a concise answer\n",
    "        answer = scibert_qa.answer_question(question, pdf['text'])\n",
    "        print(f\"SciBERT Answer: {answer}\")\n",
    "\n",
    "        # Step 2: Use T5 to generate a more detailed response\n",
    "        detailed_response = t5_generator.generate_response(question, answer)\n",
    "        print(f\"T5 Generated Response: {detailed_response}\\n\")\n",
    "\n",
    "# Run the pipeline\n",
    "folder_path = \"./Data\"  # Replace with the actual folder path containing your PDFs\n",
    "qa_pipeline(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
